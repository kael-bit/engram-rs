[package]
name = "engram"
version = "0.14.0"
edition = "2021"
description = "Hierarchical memory engine for AI agents"
license = "MIT"
repository = "https://github.com/kael-bit/engram-rs"
keywords = ["ai", "memory", "agent", "hierarchical", "cognitive"]
categories = ["database", "command-line-utilities"]
readme = "README.md"

[dependencies]
axum = { version = "0.8", features = ["default"] }
tower-http = { version = "0.6", features = ["limit"] }
tokio = { version = "1", features = ["full"] }
rusqlite = { version = "0.38", features = ["bundled"] }
serde = { version = "1", features = ["derive"] }
serde_json = "1"
uuid = { version = "1", features = ["v4"] }
clap = { version = "4", features = ["derive", "env"] }
subtle = "2"
reqwest = { version = "0.12", default-features = false, features = ["json", "rustls-tls", "stream"] }
tokio-stream = "0.1"
futures = "0.3"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
regex = "1"
thiserror = "2"
r2d2 = "0.8.10"
r2d2_sqlite = "0.32.0"
jieba-rs = "0.8"
listenfd = "1.0.2"
lru = "0.12"
parking_lot = "0.12"
hnsw_rs = "0.3"
rand = "0.10.0"
bytemuck = { version = "1", features = ["derive"] }
backon = "1"

[dev-dependencies]
tempfile = "3"
tower = { version = "0.5", features = ["util"] }
http-body-util = "0.1"

[profile.release]
opt-level = "z"
lto = true
strip = true
codegen-units = 1

# Fast deploy builds â€” use `cargo build --profile deploy`
# Skips LTO and uses more codegen units for ~3x faster compile.
# Runtime perf is identical for I/O-bound workloads like engram.
[profile.deploy]
inherits = "release"
opt-level = 2
lto = false
codegen-units = 8
